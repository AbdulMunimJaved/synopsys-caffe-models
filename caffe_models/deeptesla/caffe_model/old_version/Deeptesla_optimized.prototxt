layer {
  name: "output/y"
  type: "Parameter"
  top: "output/y"
}
layer {
  name: "x"
  type: "Input"
  top: "x"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 66
      dim: 200
    }
  }
}
layer {
  name: "Conv2D_nchw"
  type: "Convolution"
  bottom: "x"
  top: "Conv2D_nchw"
  convolution_param {
    num_output: 24
    bias_term: true
    group: 1
    kernel_h: 5
    kernel_w: 5
    stride_h: 2
    stride_w: 2
    dilation: 1
    dilation: 1
    pad_l: 0
    pad_r: 0
    pad_t: 0
    pad_b: 0
  }
}
layer {
  name: "Relu"
  type: "ReLU"
  bottom: "Conv2D_nchw"
  top: "Relu"
}
layer {
  name: "Conv2D_1_nchw"
  type: "Convolution"
  bottom: "Relu"
  top: "Conv2D_1_nchw"
  convolution_param {
    num_output: 36
    bias_term: true
    group: 1
    kernel_h: 5
    kernel_w: 5
    stride_h: 2
    stride_w: 2
    dilation: 1
    dilation: 1
    pad_l: 0
    pad_r: 0
    pad_t: 0
    pad_b: 0
  }
}
layer {
  name: "Relu_1"
  type: "ReLU"
  bottom: "Conv2D_1_nchw"
  top: "Relu_1"
}
layer {
  name: "Conv2D_2_nchw"
  type: "Convolution"
  bottom: "Relu_1"
  top: "Conv2D_2_nchw"
  convolution_param {
    num_output: 48
    bias_term: true
    group: 1
    kernel_h: 5
    kernel_w: 5
    stride_h: 2
    stride_w: 2
    dilation: 1
    dilation: 1
    pad_l: 0
    pad_r: 0
    pad_t: 0
    pad_b: 0
  }
}
layer {
  name: "Relu_2"
  type: "ReLU"
  bottom: "Conv2D_2_nchw"
  top: "Relu_2"
}
layer {
  name: "Conv2D_3_nchw"
  type: "Convolution"
  bottom: "Relu_2"
  top: "Conv2D_3_nchw"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
    dilation: 1
    pad_l: 0
    pad_r: 0
    pad_t: 0
    pad_b: 0
  }
}
layer {
  name: "Relu_3"
  type: "ReLU"
  bottom: "Conv2D_3_nchw"
  top: "Relu_3"
}
layer {
  name: "Conv2D_4_nchw"
  type: "Convolution"
  bottom: "Relu_3"
  top: "Conv2D_4_nchw"
  convolution_param {
    num_output: 64
    bias_term: true
    group: 1
    kernel_h: 3
    kernel_w: 3
    stride_h: 1
    stride_w: 1
    dilation: 1
    dilation: 1
    pad_l: 0
    pad_r: 0
    pad_t: 0
    pad_b: 0
  }
}
layer {
  name: "Relu_4"
  type: "ReLU"
  bottom: "Conv2D_4_nchw"
  top: "Relu_4"
}
layer {
  name: "Relu_4/Reshape_perm"
  type: "Permute"
  bottom: "Relu_4"
  top: "Relu_4/Reshape_perm"
  permute_param {
    order: 0
    order: 2
    order: 3
    order: 1
  }
}
layer {
  name: "Reshape"
  type: "Reshape"
  bottom: "Relu_4/Reshape_perm"
  top: "Reshape"
  reshape_param {
    shape {
      dim: -1
      dim: 1152
    }
  }
}
layer {
  name: "MatMul"
  type: "InnerProduct"
  bottom: "Reshape"
  top: "MatMul"
  inner_product_param {
    num_output: 1164
    bias_term: true
    transpose: true
  }
}
layer {
  name: "Relu_5"
  type: "ReLU"
  bottom: "MatMul"
  top: "Relu_5"
}
layer {
  name: "ExpandDims"
  type: "ExpandDimsND"
  bottom: "Relu_5"
  top: "ExpandDims"
  expand_dims_nd_param {
    axis: 1
  }
}
layer {
  name: "cell_state"
  type: "Input"
  top: "cell_state"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 100
    }
  }
}
layer {
  name: "hidden_state"
  type: "Input"
  top: "hidden_state"
  input_param {
    shape {
      dim: 1
      dim: 1
      dim: 100
    }
  }
}
layer {
  name: "rnn/basic_lstm_cell/continue"
  type: "Parameter"
  top: "rnn/basic_lstm_cell/continue"
  parameter_param {
    shape {
      dim: 1
      dim: 1
    }
  }
}
layer {
  name: "packed"
  type: "LSTM"
  bottom: "ExpandDims"
  bottom: "rnn/basic_lstm_cell/continue"
  bottom: "hidden_state"
  bottom: "cell_state"
  top: "packed"
  top: "packed:1"
  top: "packed:2"
  recurrent_param {
    num_output: 100
    expose_hidden: true
    forget_bias: 1.0
  }
}
layer {
  name: "unstack_1"
  type: "Unstack"
  bottom: "packed"
  top: "unstack_1"
  unstack_param {
    axis: 1
  }
}
layer {
  name: "MatMul_1"
  type: "InnerProduct"
  bottom: "unstack_1"
  top: "MatMul_1"
  inner_product_param {
    num_output: 50
    bias_term: true
    transpose: true
  }
}
layer {
  name: "Relu_6"
  type: "ReLU"
  bottom: "MatMul_1"
  top: "Relu_6"
}
layer {
  name: "MatMul_2"
  type: "InnerProduct"
  bottom: "Relu_6"
  top: "MatMul_2"
  inner_product_param {
    num_output: 10
    bias_term: true
    transpose: true
  }
}
layer {
  name: "Relu_7"
  type: "ReLU"
  bottom: "MatMul_2"
  top: "Relu_7"
}
layer {
  name: "MatMul_3"
  type: "InnerProduct"
  bottom: "Relu_7"
  top: "MatMul_3"
  inner_product_param {
    num_output: 1
    bias_term: true
    transpose: true
  }
}
layer {
  name: "Atan"
  type: "Atan"
  bottom: "MatMul_3"
  top: "Atan"
}
layer {
  name: "output"
  type: "Mul"
  bottom: "Atan"
  bottom: "output/y"
  top: "output"
}
